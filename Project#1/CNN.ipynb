{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":23546,"status":"ok","timestamp":1718422656486,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"pd1lDWMLgDQ_","outputId":"319dd893-aba2-40aa-8932-ebe9e7ca6959"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":7165,"status":"ok","timestamp":1718422663649,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"5xtX-3oMfVxG"},"outputs":[],"source":["### Import necessary libraries ###\n","import os\n","import torch\n","from torchvision import transforms\n","from torch.utils.data import DataLoader, random_split, Subset\n","from torchvision.models import resnet18\n","from torch.optim import Adam\n","from torch.nn import CrossEntropyLoss\n","import numpy as np\n","from tqdm import tqdm\n","\n","from sklearn.model_selection import KFold\n","from sklearn.metrics import accuracy_score, confusion_matrix\n","\n","from LoadData import CustomDataset\n","np.random.seed(42)"]},{"cell_type":"code","execution_count":3,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718422663650,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"fckELomdfdI-"},"outputs":[],"source":["### Hyperparameters ###\n","re_size = (196, 196) #(144, 260)\n","batch_size = 32\n","lr = 0.001\n","k = 3\n","num_classes = 5\n","epochs = 2"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":3,"status":"ok","timestamp":1718422663650,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"NgKE5nJXfj5g"},"outputs":[],"source":["### Define Transforms ###\n","train_transform = transforms.Compose(\n","    [\n","        # Image pre-processing: resize & randomly Horizontal-Flip & normalize\n","        transforms.Resize(re_size),\n","        transforms.RandomHorizontalFlip(),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","val_transform = transforms.Compose(\n","    [\n","        # Image pre-processing: resize & normalize\n","        transforms.Resize(re_size),\n","        transforms.ToTensor(),\n","        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n","    ])\n","\n","def custom_collate_train(batch):\n","    images, labels = zip(*batch)\n","    # Apply train_transform on images\n","    images = [train_transform(img) for img in images]\n","\n","    return torch.stack(images), torch.tensor(labels)\n","\n","def custom_collate_val(batch):\n","    images, labels = zip(*batch)\n","    # Apply val_transform on images\n","    images = [val_transform(img) for img in images]\n","\n","    return torch.stack(images), torch.tensor(labels)"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":7126,"status":"ok","timestamp":1718422670774,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"o61SdimPfnLU"},"outputs":[],"source":["### Load datasets ###\n","# Load formatted dataset from the directory \"AI_Project1/data\"\n","dataset = CustomDataset(os.path.join(os.getcwd(), 'gdrive/MyDrive/AI_Project1/data'))\n","\n","# Split the dataset into train/test subsets, with an 8:2 ratio\n","dataset_size = len(dataset)\n","train_size = int(0.8 * dataset_size)\n","test_size = dataset_size - train_size\n","\n","train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n","# Load the train/test sets\n","train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_train)\n","test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_val)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1718422670774,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"7sXbOE6jatr1","outputId":"d5138a91-756e-4c29-dd3c-8646d7baaab9"},"outputs":[],"source":["\"\"\"\n","### Apply PCA(optional) ###\n","from sklearn.decomposition import PCA\n","\n","n_components = 300\n","\n","# Define PCA model\n","pca = PCA(n_components=n_components)\n","\n","re_shape = 10\n","all_flat_imgs = []\n","# Collect training set\n","for batch_img, _ in train_loader:\n","  # Reshape the image size to match the specified input shape for PCA\n","  batch_flat_img = batch_img.reshape(batch_img.size(0), 3*re_size[0]*re_size[1])\n","  all_flat_imgs.append(batch_flat_img)\n","all_flat_imgs = torch.vstack(all_flat_imgs)\n","\n","# Fit PCA model\n","pca.fit(all_flat_imgs)\n","\"\"\""]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1718422670774,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"nJlGvpJtdTFi"},"outputs":[],"source":["# Define functions for model training and testing process\n","def train_model(model, loader, criterion, optimizer, device):\n","  model.train()\n","\n","  for (inputs, targets) in tqdm(loader, ncols=80):\n","    \"\"\"\n","    # (PCA Part)\n","    # Reshape the inputs to match the specified input shape for PCA\n","    inputs_flat = inputs.reshape(inputs.size(0), 3*re_size[0]*re_size[1])\n","    # Transfor dataset with trained PCA model\n","    inputs_flat = pca.transform(inputs_flat)\n","    # Reshape the inputs back to the specified input shape for ResNet\n","    inputs = torch.tensor(inputs_flat.reshape(inputs.size(0), 3, re_shape, re_shape))\n","    \"\"\"\n","\n","    # Set the gradients to zero\n","    optimizer.zero_grad()\n","    inputs, targets = inputs.to(device), targets.to(device)\n","\n","    # Forward propagation\n","    x = net(inputs.float())\n","    loss = criterion(x, targets)\n","    # Backward propagation\n","    loss.backward()\n","\n","    optimizer.step()\n","\n","\n","def test_model(model, loader, criterion,  device):\n","  model.eval()\n","\n","  test_loss = 0\n","  preds_history = torch.tensor([])\n","  labels_history = torch.tensor([])\n","\n","  with torch.no_grad():\n","    for inputs, targets in tqdm(loader, ncols=80):\n","      \"\"\"\n","      # (PCA Part)\n","      # (The process of applying PCA is the same as done in train_model())\n","      inputs_flat = inputs.reshape(inputs.size(0), 3*re_size[0]*re_size[1])\n","      inputs_flat = pca.transform(inputs_flat)\n","      inputs = torch.tensor(inputs_flat.reshape(inputs.size(0), 3, re_shape, re_shape))\n","      \"\"\"\n","\n","      inputs, targets = inputs.to(device), targets.to(device)\n","\n","      # Make prediciton\n","      x = net(inputs.float())\n","      _, predicted = torch.max(x.data, 1)\n","      # Compute loss\n","      loss = criterion(x, targets)\n","      test_loss += loss.item()\n","\n","      preds_history = torch.cat((preds_history, predicted.cpu()), dim=0)\n","      labels_history = torch.cat((labels_history, targets.cpu()), dim=0)\n","\n","\n","  # Compute the accuracy and confusion matrix\n","  acc = accuracy_score(labels_history.cpu().numpy(), preds_history.cpu().numpy())\n","  cnf = confusion_matrix(labels_history.cpu().numpy(), preds_history.cpu().numpy())\n","\n","  return test_loss/len(loader), acc, cnf"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":913876,"status":"ok","timestamp":1718423584642,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"lIBBR3mVk0a0","outputId":"b834a15a-059e-4539-da9b-2a1753c98cec"},"outputs":[],"source":["### Train ResNet ###\n","# Define K-fold cross-validation model\n","kf = KFold(n_splits=k,shuffle=True)\n","\n","# K-fold training iterations\n","per_fold_result={}\n","best_acc = 0.0\n","best_model_path = os.path.join(os.getcwd(), 'gdrive/MyDrive/AI_Project1/result/CNN/resnet18.pth')\n","for i, (train_idx,val_idx) in enumerate(kf.split(np.arange(len(train_dataset)))):\n","  print(\"Fold no.{}:\".format(i + 1))\n","\n","  # Training and testing subset of this fold\n","  train_subset = Subset(train_dataset, train_idx)\n","  val_subset = Subset(train_dataset, val_idx)\n","  subtrain_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True, num_workers=2, collate_fn=custom_collate_train)\n","  subval_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False, num_workers=2, collate_fn=custom_collate_val)\n","\n","\n","  ### Model Setting ###\n","  # Call the pretrained model and adjust its fully-connected layer to suit my case\n","  net = resnet18(pretrained=True)\n","  net.fc = torch.nn.Linear(net.fc.in_features, num_classes)\n","\n","  # Set device\n","  device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","  net.to(device)\n","\n","  # Enable gradient computation for all parameters in the model\n","  for param in net.parameters():\n","    param.requires_grad = True\n","\n","  # Define loss function and optimizer\n","  criterion = CrossEntropyLoss()\n","  optimizer = Adam(net.parameters(), lr=lr)\n","\n","\n","  # n-epochs training iterations\n","  history = {'val_loss': [], 'val_acc':[]}\n","  for epoch in range(epochs):\n","    # Fit the model on the current training subset\n","    train_model(net, subtrain_loader, criterion, optimizer, device)\n","    # Make prediction on the current testing subset\n","    val_loss, val_acc, _ = test_model(net, subval_loader, criterion, device)\n","    print(\"Testing Epoch {} | loss: {:.4f} | Accuracy:{:.4f}\".format(epoch+1, val_loss/len(subval_loader), val_acc))\n","\n","    history['val_loss'].append(val_loss)\n","    history['val_acc'].append(val_acc)\n","\n","  per_fold_result['fold{}'.format(i+1)] = history\n","\n","\n","  # Save the current model if it is the one with the best performance so far\n","  if history['val_acc'][epochs-1]>best_acc:\n","    best_acc = history['val_acc'][epochs-1]\n","    torch.save(net.state_dict(), best_model_path)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":49268,"status":"ok","timestamp":1718423633907,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"07rInSgk8X6R","outputId":"848b14ca-b76b-491f-bb52-022af1b40be6"},"outputs":[],"source":["# Load the best trained model\n","best_net = resnet18()\n","# (The model setting process following is the same as before)\n","best_net.fc = torch.nn.Linear(best_net.fc.in_features, num_classes)\n","best_net.load_state_dict(torch.load(best_model_path))\n","\n","best_net.to(device)\n","\n","for param in best_net.parameters():\n","  param.requires_grad = True\n","\n","criterion = CrossEntropyLoss()\n","optimizer = Adam(best_net.parameters(), lr=lr)\n","\n","# Retrain the model with the whole training set\n","train_model(best_net, train_loader, criterion, optimizer, device)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":169160,"status":"ok","timestamp":1718423803064,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"8TYYi4IZ-eSX","outputId":"5d96fc87-819b-4621-df25-2cda4b65eaef"},"outputs":[],"source":["### Test ###\n","# Make prediction with trained model\n","final_loss, final_acc, final_cnf = test_model(best_net, test_loader, criterion, device)\n","\n","# Show the prediciton result\n","print(\"Final loss: {:.4f} | Accuracy: {:.4f}\".format(final_loss/len(test_loader), final_acc))\n","print(\"\\nConfusion_matrix:\\n{}\".format(final_cnf))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"output_embedded_package_id":"1YD5ERnkp0pZO0cbdlKxQkfnbUyqeugTf"},"executionInfo":{"elapsed":14138,"status":"ok","timestamp":1718423911391,"user":{"displayName":"林怡秀","userId":"08690569950036176555"},"user_tz":-480},"id":"nnKq4_JpTyb2","outputId":"1f0d599a-4332-4c41-f297-7f639959afcc"},"outputs":[],"source":["### Show results ###\n","import matplotlib.pyplot as plt\n","\n","int2str = {0: 'Crayon_Shin', 1: 'Doraemon', 2: 'Hua_Family', 3: 'Ilu', 4: 'Maruko'}\n","\n","# Show the image and its prediction\n","for i in range(10):\n","  image, label = test_dataset[i]\n","  image_trans = val_transform(image)\n","  image_trans = image_trans.to(device)\n","\n","  x = best_net(image_trans.unsqueeze(0))\n","  _, predicted = torch.max(x.data, 1)\n","\n","  plt.imshow(image)\n","  plt.text(0, -0.1, \"Pred : {}\\nLabel: {}\".format(int2str[predicted.cpu().item()], int2str[label]), transform=plt.gca().transAxes)\n","  plt.axis('off')\n","\n","  plt.savefig(os.path.join(os.getcwd(), f'gdrive/MyDrive/AI_Project1/result/CNN/prediction_{i+1}.jpg'))\n","  plt.show()\n","\n","  plt.clf()"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
